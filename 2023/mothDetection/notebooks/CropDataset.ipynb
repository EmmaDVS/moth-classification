{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2df8aa5",
   "metadata": {},
   "source": [
    "# Crop a dataset with YOLOv5\n",
    "\n",
    "After training a custom YOLOv5 model in MothDetectionYOLOv5.ipynb, it is time to put it into practice. In this notebook, a dataset is fed to the cropping model. The images of the crops around the objects (moths) are saved as its result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f852c4ee",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be58fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from PIL import Image, ImageOps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56f27e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '/data/mothRecognition/data/meetnet_230911_231019'  # Location of the to-be-cropped dataset\n",
    "save_dir = '/data/croppedDatasetNew/'  # Location to save the crops to\n",
    "yolo_weights_path = '/data/mothDetection/yolov5/runs/train/exp3/weights/best.pt'  # Location of the custom YOLOv5 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1fc2327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/farfalla/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2023-10-12 Python-3.8.18 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 4090, 24209MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "yolo = torch.hub.load('ultralytics/yolov5', 'custom', path=yolo_weights_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d613d877",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "015621ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "        \n",
    "def squareAndPadBox(box):\n",
    "    # box = list(box)\n",
    "    # Make the crop square by elongating the shortest side\n",
    "    w = box[2] - box[0]\n",
    "    h = box[3] - box[1]\n",
    "\n",
    "    pad_for_square = (max(w, h) - min(w, h)) / 2\n",
    "    shortest_side = 'w' if w < h else 'h'\n",
    "\n",
    "    if shortest_side == 'w':\n",
    "        box[0] = box[0] - pad_for_square\n",
    "        box[2] = box[2] + pad_for_square\n",
    "    if shortest_side == 'h':\n",
    "        box[1] = box[1] - pad_for_square\n",
    "        box[3] = box[3] + pad_for_square\n",
    "\n",
    "    # Pad with 10% on all sides\n",
    "    pad = 0.1 * (box[2] - box[0])\n",
    "    box[0] = box[0] - pad\n",
    "    box[1] = box[1] - pad\n",
    "    box[2] = box[2] + pad\n",
    "    box[3] = box[3] + pad\n",
    "    return box\n",
    "\n",
    "def squareYoloCrop(images,\n",
    "                   predictions,\n",
    "                   filenames,\n",
    "                   save_dir,\n",
    "                   size=240,\n",
    "                   center_cropped=0,\n",
    "                   multi_crop=False,\n",
    "                   min_box_score=0.5):\n",
    "    \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    for i, image in enumerate(images):\n",
    "        prediction = predictions[i]\n",
    "        boxes = prediction[:, :4].cpu().tolist() # x1, y1, x2, y2\n",
    "        scores = prediction[:, 4]\n",
    "        box_crops = []\n",
    "        boxes_valid = len(boxes) >= 1 and max(scores) >= min_box_score\n",
    "        crop_scores = []\n",
    "        \n",
    "        if not boxes_valid:  # Do a centre crop instead\n",
    "            center_cropped += 1\n",
    "            w = image.width\n",
    "            h = image.height\n",
    "            \n",
    "            if w > h:\n",
    "                left = int(w/2-h/2)\n",
    "                upper = 0\n",
    "                right = left + h\n",
    "                lower = h\n",
    "            if h > w:\n",
    "                left = 0\n",
    "                upper = int(h/2-w/2)\n",
    "                right = w\n",
    "                lower = upper + w\n",
    "            if w == h:\n",
    "                left = 0\n",
    "                upper = 0\n",
    "                right = w\n",
    "                lower = h\n",
    "            box = (left, upper, right, lower)\n",
    "            box_crops.append(box)\n",
    "            crop_scores.append('centre')\n",
    "\n",
    "        if boxes_valid:\n",
    "            if multi_crop:\n",
    "                for score_i, score in enumerate(scores):\n",
    "                    if score >= min_box_score:\n",
    "                        box = boxes[score_i]\n",
    "                        box_crops.append(box)\n",
    "                        crop_scores.append(score)\n",
    "            else:\n",
    "                best_crop_index = torch.argmax(scores).tolist()  # Index with highest score\n",
    "                box = boxes[best_crop_index]\n",
    "                box_crops.append(box)\n",
    "                crop_scores.append(scores[best_crop_index])\n",
    "            box_crops = list(map(squareAndPadBox, box_crops))\n",
    "        \n",
    "        for box_i, box in enumerate(box_crops):\n",
    "            crop = image.crop(box)\n",
    "            crop = crop.resize([size, size])\n",
    "\n",
    "            filename_and_extension = str.split(str.split(filenames[i], \"/\")[-1], \".\")\n",
    "            if len(filename_and_extension) == 2:\n",
    "                if boxes_valid:\n",
    "                    filename = f\"{crop_scores[box_i]:.2f}_{filename_and_extension[0]}_{box_i}.{filename_and_extension[1]}\"\n",
    "                else:\n",
    "                    filename = f\"{crop_scores[box_i]}_{filename_and_extension[0]}_{box_i}.{filename_and_extension[1]}\"\n",
    "            else:\n",
    "                print(f\"Failed saving crop: Filename and extension include 0 or more than 1 period ('.'): {filename_and_extension}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                crop.save(save_dir + filename)\n",
    "            except ValueError:\n",
    "                print(f\"ValueError for {filename}, not saved.\")\n",
    "            except:\n",
    "                print(\"Something undefined went wrong.\")\n",
    "            \n",
    "    return center_cropped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0145e7b",
   "metadata": {},
   "source": [
    "## Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57e08321",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir 254 out of 254/batch 0 (batch size = 512)/image 4338 out of 4338/center cropped = 112\n",
      "\n",
      "214.71734714508057\n",
      "faulty batches: []\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "faulty_batches = []\n",
    "nr_of_processed_images = 0\n",
    "center_cropped = 0\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i, (sub_dir, dirs, files) in enumerate(os.walk(dataset_dir)):\n",
    "    if i == 0 and len(files) == 0:\n",
    "        nr_of_dirs = len(dirs)\n",
    "    if len(files) > 0:\n",
    "        sub_dir_lowest = str.split(sub_dir, \"/\")[-1]\n",
    "        paths = [sub_dir + \"/\" + file for file in files]\n",
    "        batches = chunks(paths, batch_size)\n",
    "        for j, batch in enumerate(batches):\n",
    "            nr_of_processed_images += len(batch)\n",
    "            print(f'Dir {i} out of {nr_of_dirs}/batch {j} (batch size = {batch_size})/image {nr_of_processed_images}/center cropped = {center_cropped}', end='\\r')\n",
    "            image_list = []\n",
    "            for filename in batch:\n",
    "                im = Image.open(filename)\n",
    "                try:\n",
    "                    ImageOps.exif_transpose(im, in_place=True)\n",
    "                except:\n",
    "                    print(\"Problem with reading EXIF data.\")\n",
    "                image_list.append(im)\n",
    "            try:\n",
    "                results = yolo(image_list).pred\n",
    "            except:\n",
    "                faulty_batches.append({'batch_size': batch_size,\n",
    "                                       'nr_of_processed_images': nr_of_processed_images,\n",
    "                                       'dir (i)': i,\n",
    "                                       'batch (j)': j})\n",
    "                print(f\"\\nError in this batch, try later (batch {j} out of dir {i} with batch_size={batch_size}) (OOM?)\")\n",
    "                continue\n",
    "            center_cropped = squareYoloCrop(images=image_list,\n",
    "                                            predictions=results,\n",
    "                                            filenames=batch,\n",
    "                                            save_dir=save_dir + sub_dir_lowest + \"/\",\n",
    "                                            center_cropped=center_cropped,\n",
    "                                            multi_crop=True,\n",
    "                                            min_box_score=25.0000)\n",
    "end = time.time()\n",
    "\n",
    "print(\"\\n\")\n",
    "print('Time taken:', end - start, 's')\n",
    "print('faulty batches:', faulty_batches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa5407b",
   "metadata": {},
   "source": [
    "If something has gone wrong in one of the batches (that is, when `faulty batches` is not empty), you can process those batches one by one by copying the loop above and only processing the faulty batch. For this, set `batch_size = 1`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "yolo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
